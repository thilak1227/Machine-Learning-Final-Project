# -*- coding: utf-8 -*-
"""Another copy of Finalized ML Project Latest

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SanBPlf5cLPsHZgSKKqGQK1l2dYzJ40u

**Goals:** *Predict patient has kidney disease using clinical attributes*

# **1.0 Import Libraries**
"""

!pip install kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""# **2.0 Load Dataset**"""

!kaggle datasets download -d mansoordaku/ckdisease --force
!unzip -o ckdisease.zip

"""# 2.1 Read CSV"""

df = pd.read_csv("kidney_disease.csv")

"""# 2.2 View CSV"""

df.head()

"""# **3.0 Explore Dataset (EDA)**"""

print(df.info())
print(df.describe())
print(df.isnull().sum())   # Check missing values
print(df.duplicated().sum()) # Check duplicates

"""# 3.1 Plot Target Distribution"""

import matplotlib.pyplot as plt
import seaborn as sns

sns.countplot(x='classification', data=df)
plt.title("Kidney Disease Distribution")
plt.show()

"""# **4.0 Preprocessing**

# 4.1 Drop Duplicates
"""

df = df.drop_duplicates()

"""# 4.2 Encode Categorical Variables"""

from sklearn.preprocessing import LabelEncoder


categorical_cols = ['rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane', 'classification']

for col in categorical_cols:
    if col in df.columns and df[col].dtype == 'object':
        df[col] = df[col].fillna('missing')
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])

"""# 4.3 Define Features & Condition"""

# Convert 'pcv', 'wc', 'rc' to numeric, coercing errors to NaN
df['pcv'] = pd.to_numeric(df['pcv'], errors='coerce')
df['wc'] = pd.to_numeric(df['wc'], errors='coerce')
df['rc'] = pd.to_numeric(df['rc'], errors='coerce')

# Impute missing numerical values with the median
# This includes NaNs created by 'coerce' above and other original NaNs
for col in ['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wc', 'rc']:
    if col in df.columns and df[col].isnull().any():
        df[col] = df[col].fillna(df[col].median())

# Drop 'id' column as it is an identifier and not a feature
X = df.drop(['classification', 'id'], axis=1)
y = df['classification']

"""# 4.4 Train Test Split"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""# 4.5 Scale Features"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""# **5.0 Train Models**

# 5.1 Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)

"""# 5.2 Random Forest"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

"""# 5.3 SVM"""

from sklearn.svm import SVC

svm = SVC(probability=True, random_state=42)
svm.fit(X_train, y_train)

"""# 5.4 XGBoost"""

import xgboost as xgb
xg = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xg.fit(X_train, y_train)

"""# **6.0 Model Evaluation**"""

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report

models = {'Logistic Regression': lr, 'Random Forest': rf, 'SVM': svm, 'XGBoost': xg}

for name, model in models.items():
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:,1]

    print(f"\nðŸ”¹ {name} Results:")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred, pos_label=2))
    print("ROC-AUC:", roc_auc_score(y_test, y_proba))
    print(classification_report(y_test, y_pred))

"""# **7.0 Confusion Matrices**"""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

for ax, (name, model) in zip(axes.flatten(), models.items()):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
    ax.set_title(f"{name} Confusion Matrix")
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")

plt.tight_layout()
plt.show()

"""# **8.0 ROC Curves**"""

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))

for name, model in models.items():
    y_proba = model.predict_proba(X_test)[:,1]
    # Specify pos_label=2 for roc_curve as y_test contains labels 0 and 2
    fpr, tpr, _ = roc_curve(y_test, y_proba, pos_label=2)
    # Remove pos_label from roc_auc_score call within the label string
    plt.plot(fpr, tpr, label=f"{name} (AUC={roc_auc_score(y_test, y_proba):.2f})")

plt.plot([0,1],[0,1],'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves")
plt.legend()
plt.show()

"""# **9.0 Features Importance**"""

importances = rf.feature_importances_
features = X.columns

plt.figure(figsize=(8,6))
sns.barplot(x=importances, y=features)
plt.title("Random Forest Feature Importance")
plt.show()

xg_importances = xg.feature_importances_

plt.figure(figsize=(8,6))
sns.barplot(x=xg_importances, y=features)
plt.title("XGBoost Feature Importance")
plt.show()

"""# **10.0 Hyper Tuning**

# 10.1 Decision Tree Hyper Tuning
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

dt_params = {
    "max_depth": [3, 5, 7, None],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}
dt_grid = GridSearchCV(DecisionTreeClassifier(random_state=42),
                       param_grid=dt_params,
                       cv=5,
                       scoring="accuracy")
dt_grid.fit(X_train, y_train)

"""# 10.2 Random Forest Tuning"""

rf_params = {
    "n_estimators": [100, 200],
    "max_depth": [3, 5, 7, None],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}
rf_grid = GridSearchCV(RandomForestClassifier(random_state=42),
                       param_grid=rf_params,
                       cv=5,
                       scoring="accuracy",
                       n_jobs=-1)
rf_grid.fit(X_train, y_train)

"""# **11.0 Tuned Model Evaluation**"""

from sklearn.model_selection import cross_val_score
import numpy as np

tuned_models = {
    "Tuned Decision Tree": dt_grid.best_estimator_,
    "Tuned Random Forest": rf_grid.best_estimator_
}

results = {} # Initialize results dictionary

for name, model in tuned_models.items():
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    cv_scores = cross_val_score(model, X_train, y_train, cv=5)

    results[name] = {
        "Train Accuracy": round(train_acc, 4),
        "Test Accuracy": round(test_acc, 4),
        "CV Mean Accuracy": round(np.mean(cv_scores), 4),
        "CV Std Dev": round(np.std(cv_scores), 4)
    }

    print(f"\n{name} - Best Params: {model.get_params()}")

"""# **12.0 Comparison**"""

print("\n=== Final Model Comparison ===")
comparison_df = pd.DataFrame(results).T
print(comparison_df)